import os
import json
import time
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from concurrent.futures import ThreadPoolExecutor, as_completed


def get_text(soup, selector, is_single=True):
    """æå–é€‰æ‹©å™¨åŒ¹é…çš„æ–‡æœ¬å†…å®¹"""
    try:
        if is_single:
            elem = soup.select_one(selector)
            return elem.get_text(strip=True) if elem else "æœªè·å–åˆ°æ•°æ®"
        else:
            elems = soup.select(selector)
            return [el.get_text(strip=True) for el in elems] if elems else []
    except Exception as e:
        print(f"è§£æ [{selector}] å¤±è´¥ï¼š{e}")
        return "" if is_single else []


def download_image(image_url, save_dir, dish_id):
    """ä¸‹è½½å›¾ç‰‡å¹¶ä»¥èœå“IDå‘½å"""
    os.makedirs(save_dir, exist_ok=True)
    filename = f"{dish_id}.jpg" if str(dish_id).strip() else "none.jpg"
    save_path = os.path.join(save_dir, filename)

    try:
        with requests.Session() as session:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
            }
            resp = session.get(image_url, headers=headers, timeout=15, stream=True)
            resp.raise_for_status()

            with open(save_path, "wb") as f:
                for chunk in resp.iter_content(chunk_size=8192):
                    f.write(chunk)
        return save_path
    except Exception as e:
        print(f"âŒ å›¾ç‰‡ä¸‹è½½å¤±è´¥ï¼ˆID:{dish_id}ï¼‰ï¼š{e}")
        return ""


def save_to_json(data, filename):
    """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“¥ å·²ä¿å­˜åˆ°ï¼š{filename}")


def init_driver():
    """åˆå§‹åŒ–Chromeæµè§ˆå™¨é…ç½®ï¼ˆæ¯ä¸ªçº¿ç¨‹ç‹¬ç«‹è°ƒç”¨ï¼‰"""
    options = webdriver.ChromeOptions()
    options.add_argument("--headless=new")  # æ— å¤´æ¨¡å¼
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument(
        "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
    )
    prefs = {
        "profile.managed_default_content_settings.images": 2,  # ç¦ç”¨å›¾ç‰‡åŠ è½½ï¼Œæé«˜é€Ÿåº¦
        "profile.managed_default_content_settings.plugins": 2,
        "profile.managed_default_content_settings.popups": 2,
        "profile.managed_default_content_settings.geolocation": 2,
        "profile.managed_default_content_settings.notifications": 2,
    }
    options.add_experimental_option("prefs", prefs)
    options.page_load_strategy = 'eager'  # åªç­‰å¾…DOMåŠ è½½å®Œæˆï¼Œä¸ç­‰å¾…èµ„æºåŠ è½½

    driver = webdriver.Chrome(
        service=Service(ChromeDriverManager().install()),
        options=options
    )

    driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
        "source": """
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            })
        """
    })

    driver.set_page_load_timeout(10)
    driver.set_script_timeout(10)

    return driver


def login_driver(driver, username, password):
    """ä¸ºå•ä¸ªdriveræ‰§è¡Œç™»å½•æ“ä½œï¼ˆæ¯ä¸ªçº¿ç¨‹ç‹¬ç«‹ç™»å½•ï¼‰"""
    try:
        driver.get("https://nutridata.cn/login")
        WebDriverWait(driver, 10).until(
            lambda d: d.execute_script('return document.readyState') == 'complete'
        )

        # å¯†ç ç™»å½•æµç¨‹
        WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.LINK_TEXT, "å¯†ç ç™»å½•"))).click()
        WebDriverWait(driver, 5).until(
            EC.visibility_of_element_located((By.CSS_SELECTOR, 'input[placeholder="è¯·è¾“å…¥ç”¨æˆ·åæˆ–æ‰‹æœºå·"]'))).send_keys(
            username)
        WebDriverWait(driver, 5).until(
            EC.visibility_of_element_located((By.CSS_SELECTOR, 'input[placeholder="è¯·è¾“å…¥å¯†ç "]'))).send_keys(password)
        WebDriverWait(driver, 5).until(EC.element_to_be_clickable(
            (By.XPATH, '//button[contains(@class, "primary-btn") and .//span[text()="ç™» å½•"]]'))).click()

        # éªŒè¯ç™»å½•æˆåŠŸ
        WebDriverWait(driver, 10).until(lambda d: "login" not in d.current_url.lower())
        print(f"âœ… çº¿ç¨‹ç™»å½•æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ çº¿ç¨‹ç™»å½•å¤±è´¥ï¼š{e}")
        return False


def process_dish_batch(args):
    """å¤„ç†ä¸€æ‰¹èœå“æ•°æ®æå–ï¼ˆæ¯ä¸ªçº¿ç¨‹ç™»å½•ä¸€æ¬¡å¤„ç†500ä¸ªIDï¼‰"""
    batch_id, dish_ids, username, password = args  # æ¥æ”¶å‚æ•°ï¼šæ‰¹æ¬¡IDã€èœå“IDåˆ—è¡¨ã€è´¦å·ã€å¯†ç 
    driver = None
    batch_results = []

    # è®°å½•æ‰¹æ¬¡å¼€å§‹æ—¶é—´ï¼ˆä»…ç”¨äºæ§åˆ¶å°è¾“å‡ºï¼‰
    start_time_ts = time.time()

    try:
        # æ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„driverå¹¶ç™»å½•ä¸€æ¬¡
        driver = init_driver()
        if not login_driver(driver, username, password):
            # ç™»å½•å¤±è´¥ï¼Œä¸ºè¯¥æ‰¹æ¬¡æ‰€æœ‰IDè®°å½•é”™è¯¯
            for dish_id in dish_ids:
                batch_results.append({
                    "èœå“ID": dish_id,
                    "é”™è¯¯ä¿¡æ¯": "ç™»å½•å¤±è´¥",
                    "å›¾ç‰‡URL": "",
                    "æœ¬åœ°ä¿å­˜è·¯å¾„": ""
                })
            return batch_results

        print(f"\n===== å¼€å§‹å¤„ç†æ‰¹æ¬¡ {batch_id}ï¼Œå…± {len(dish_ids)} ä¸ªèœå“ =====")

        # å¤„ç†æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªèœå“ID
        for idx, dish_id in enumerate(dish_ids, 1):
            try:
                url = f"https://nutridata.cn/database/dishes/{dish_id}"
                print(f"[{batch_id}æ‰¹æ¬¡-{idx}/{len(dish_ids)}] å¤„ç†èœå“ ID: {dish_id} | URL: {url}")

                try:
                    driver.get(url)
                except:
                    print(f"é¡µé¢åŠ è½½è¶…æ—¶ï¼Œå°è¯•ç»§ç»­å¤„ç† ID: {dish_id}")

                # ç­‰å¾…å…³é”®å…ƒç´ 
                try:
                    WebDriverWait(driver, 3).until(
                        EC.visibility_of_element_located((By.CSS_SELECTOR, ".info-title.ellipsis-1"))
                    )
                except:
                    print(f"æ ¸å¿ƒå…ƒç´ åŠ è½½è¶…æ—¶ï¼Œå°è¯•ç»§ç»­å¤„ç† ID: {dish_id}")

                # æå–å›¾ç‰‡ä¿¡æ¯
                img_url = "æœªè·å–åˆ°å›¾ç‰‡URL"
                img_local_path = ""
                try:
                    img_elem = WebDriverWait(driver, 2).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "span.el-link--inner img"))
                    )
                    img_url = img_elem.get_attribute("src") or img_url
                    if img_url.startswith("http"):
                        img_local_path = download_image(img_url, "dish_images", dish_id)
                except:
                    pass

                # å±•å¼€å•ä½ä¸‹æ‹‰èœå•
                try:
                    dropdown = WebDriverWait(driver, 2).until(
                        EC.element_to_be_clickable((By.CSS_SELECTOR, 'div.unit-select .el-select__caret'))
                    )
                    dropdown.click()
                    WebDriverWait(driver, 2).until(
                        EC.visibility_of_element_located(
                            (By.CSS_SELECTOR, ".el-select-dropdown__list .el-select-dropdown__item"))
                    )
                except:
                    pass

                # è§£æé¡µé¢æ•°æ®
                soup = BeautifulSoup(driver.page_source, "html.parser")
                ingredients = get_text(soup, ".ingredients span")
                steps = get_text(soup, ".practice-step", is_single=False)

                batch_results.append({
                    "èœå“ID": dish_id,
                    "èœå“åç§°": get_text(soup, ".info-title.ellipsis-1"),
                    "æˆåˆ†":"\n".join([i.replace("\n", " ").strip() for i in
                                      get_text(soup, ".info-tag .tag-item", False)]),
                    "è®¡é‡å•ä½": get_text(soup, ".title-tip"),
                    "å›¾ç‰‡URL": img_url,
                    "æœ¬åœ°å›¾ç‰‡è·¯å¾„": img_local_path,
                    "èœè‚´åšæ³•": f"{ingredients}\n" + "\n".join(steps) if (ingredients or steps) else "æœªè·å–åˆ°åšæ³•",
                    "èƒ½é‡åŠå®é‡è¥å…»ç´ ": "\n".join([i.replace("\n", " ").strip() for i in
                                                   get_text(soup, ".chart-item.color-class-0 .item-chart-outer",
                                                            False)]),
                    "ç»´ç”Ÿç´ ": "\n".join([i.replace("\n", " ").strip() for i in
                                         get_text(soup, ".chart-item.color-class-1 .item-chart-outer", False)]),
                    "çŸ¿ç‰©è´¨": "\n".join([i.replace("\n", " ").strip() for i in
                                         get_text(soup, ".chart-item.color-class-2 .item-chart-outer", False)]),
                    "å•ä½é‡": "\n".join(
                        get_text(soup, ".el-select-dropdown__list .el-select-dropdown__item", False)) or "æœªè·å–åˆ°å•ä½é‡"
                })

            except Exception as e:
                error = f"å¤„ç†å¤±è´¥: {e}"
                print(f"âŒ {error}")
                batch_results.append({
                    "èœå“ID": dish_id,
                    "é”™è¯¯ä¿¡æ¯": error,
                    "å›¾ç‰‡URL": "",
                    "æœ¬åœ°ä¿å­˜è·¯å¾„": ""
                })

        # è®°å½•æ‰¹æ¬¡ç»“æŸæ—¶é—´ï¼ˆä»…ç”¨äºæ§åˆ¶å°è¾“å‡ºï¼‰
        end_time_ts =time.time()
        duration = round(end_time_ts - start_time_ts, 2)
        print(f"===== æ‰¹æ¬¡ {batch_id} å®Œæˆ | æ€»è€—æ—¶ï¼š{duration} ç§’ =====")

        return batch_results

    except Exception as e:
        error = f"æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}"
        print(f"âŒ {error}")
        # ä¸ºè¯¥æ‰¹æ¬¡å‰©ä½™æœªå¤„ç†çš„IDè®°å½•é”™è¯¯
        for dish_id in dish_ids[len(batch_results):]:
            batch_results.append({
                "èœå“ID": dish_id,
                "é”™è¯¯ä¿¡æ¯": error,
                "å›¾ç‰‡URL": "",
                "æœ¬åœ°ä¿å­˜è·¯å¾„": ""
            })
        return batch_results
    finally:
        if driver:
            driver.quit()  # æ‰¹æ¬¡å¤„ç†å®Œæˆåå…³é—­driver


def crawl_dish_data(start_id, end_id, username, password, batch_size=100, max_workers=3):
    """æ‰¹é‡çˆ¬å–èœå“æ•°æ®ï¼ˆæ¯æ¬¡ç™»å½•å¤„ç†100æ¡æ•°æ®ï¼‰"""
    all_data = []
    total = end_id - start_id + 1
    print(f"ğŸš€ å¼€å§‹çˆ¬å– [{start_id}-{end_id}]ï¼Œå…± {total} æ¡æ•°æ®ï¼Œæ¯æ‰¹å¤„ç† {batch_size} æ¡ï¼Œçº¿ç¨‹æ•°ï¼š{max_workers}")

    # ç”Ÿæˆæ‰€æœ‰èœå“ID
    all_dish_ids = list(range(start_id, end_id + 1))

    # åˆ†æˆå¤šä¸ªæ‰¹æ¬¡
    batches = []
    for i in range(0, len(all_dish_ids), batch_size):
        batch_ids = all_dish_ids[i:i + batch_size]
        batches.append((len(batches) + 1, batch_ids))  # (æ‰¹æ¬¡å·, è¯¥æ‰¹æ¬¡çš„IDåˆ—è¡¨)

    print(f"ğŸ“¦ å…±åˆ†ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡")

    # å‡†å¤‡ä»»åŠ¡å‚æ•°ï¼šæ¯ä¸ªä»»åŠ¡åŒ…å«ï¼ˆæ‰¹æ¬¡å·, è¯¥æ‰¹æ¬¡çš„IDåˆ—è¡¨, ç”¨æˆ·å, å¯†ç ï¼‰
    task_args = [(batch_id, batch_ids, username, password) for batch_id, batch_ids in batches]

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰æ‰¹æ¬¡ä»»åŠ¡
        futures = {executor.submit(process_dish_batch, args): args[0] for args in task_args}

        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        for future in as_completed(futures):
            batch_id = futures[future]
            try:
                batch_data = future.result()
                all_data.extend(batch_data)

                # å®æ—¶ä¿å­˜è¿›åº¦
                success = len([d for d in all_data if 'é”™è¯¯ä¿¡æ¯' not in d])
                processed = len(all_data)
                print(f"\nğŸ“Š æ€»è¿›åº¦ï¼š{processed}/{total} | æˆåŠŸï¼š{success} | å¤±è´¥ï¼š{processed - success}")
                save_to_json(all_data, "dishes_data_progress.json")
            except Exception as e:
                print(f"å¤„ç†æ‰¹æ¬¡ {batch_id} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")

                # è®°å½•è¯¥æ‰¹æ¬¡æ‰€æœ‰IDçš„é”™è¯¯
                batch_ids = next(b[1] for b in batches if b[0] == batch_id)
                for dish_id in batch_ids:
                    all_data.append({
                        "èœå“ID": dish_id,
                        "é”™è¯¯ä¿¡æ¯": f"æ‰¹æ¬¡å¤„ç†å¼‚å¸¸: {e}"
                    })

    return all_data


if __name__ == "__main__":
    USERNAME = ""  # https://nutridata.cn/çš„è´¦å·
    PASSWORD = ""  # https://nutridata.cn/çš„å¯†ç 
    START_ID = 8456  # èµ·å§‹ID
    END_ID = 34123  # ç»“æŸID
    BATCH_SIZE = 500  # æ¯æ‰¹å¤„ç†çš„æ•°é‡
    MAX_WORKERS = 1  # çº¿ç¨‹æ•°ï¼ˆä¸å®œè¿‡å¤šï¼Œé¿å…è§¦å‘åçˆ¬ï¼‰

    result = crawl_dish_data(START_ID, END_ID, USERNAME, PASSWORD, BATCH_SIZE, MAX_WORKERS)
    save_to_json(result, "dishes_data_complete.json")

    success_count = len([d for d in result if 'é”™è¯¯ä¿¡æ¯' not in d])
    print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼æ€»æ•°é‡ï¼š{len(result)} | æˆåŠŸï¼š{success_count} | å¤±è´¥ï¼š{len(result) - success_count}")
